global_seed: 17
max_seq_len: 2048
run_name: mpt-passes-1.0-ref-125M-26B
model:
  name: mpt_causal_lm
  init_device: meta
  d_model: 768
  n_heads: 12
  n_layers: 12
  expansion_ratio: 4
  max_seq_len: ${max_seq_len}
  vocab_size: 100352
  tokenizer_name: ${tokenizer_name}
  no_bias: true
  norm_type: low_precision_layernorm
  emb_pdrop: 0
  resid_pdrop: 0
  init_config:
    init_nonlinearity: relu
    name: kaiming_normal_
  attn_config:
    alibi: true
    attn_impl: triton
    clip_qkv: 6
    attn_uses_sequence_id: false
    attn_pdrop: 0
tokenizer_name: tiktoken
tokenizer:
  name: ${tokenizer_name}
  kwargs:
    model_name: gpt-4
global_train_batch_size: 512
max_duration: 26000000000tok
optimizer:
  name: decoupled_lionw
  lr: 0.0002
  betas:
  - 0.9
  - 0.95
  weight_decay: 0.0002
scheduler:
  name: cosine_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.1
seed: ${global_seed}
precision: amp_bf16
device_train_microbatch_size: 8
device_eval_batch_size: 8
fsdp_config:
  activation_checkpointing: false
  activation_checkpointing_reentrant: false
  activation_cpu_offload: false
  limit_all_gathers: true
  mixed_precision: PURE
  sharding_strategy: FULL_SHARD
  state_dict_type: full
  verbose: false
eval_first: false
eval_interval: 0.1dur
log_to_console: true
console_log_interval: 100ba
progress_bar: false
python_log_level: DEBUG
loggers:
  wandb:
    project: neural-data-filtering
    tags:
    - seed-${global_seed}
    - dataset-mpt
    - seed-17
    - lr-0.0002
    - ref
    - ref-params-125M
    - ref-tok-26B
    - num-passes-1.0
    - tokenizer-gpt4-tiktoken
    - seq-len-2048
    group: mpt-passes-1.0-ref-125M-26B
    name: mpt-passes-1.0-ref-125M-26B-sd-17
autoresume: true
save_filename: ba{batch}/rank{rank}.pt
save_folder: oci://mosaicml-internal-checkpoints/zack/me-fomo-data-filtering/mpt/gpt4-tiktoken-seqlen-2048/reference/mpt-passes-1.0-ref-125M-26B-bs-512-lr-0.0002-sd-17/ckpts
save_interval: 500ba
save_num_checkpoints_to_keep: 1
algorithms:
  gradient_clipping:
    clipping_threshold: 1
    clipping_type: norm
callbacks:
  lr_monitor: {}
  memory_monitor: {}
  runtime_estimator: {}
  scheduled_gc:
    batch_interval: 2000
  speed_monitor:
    window_size: 1
  gpu_hour_logger: {}
eos_token_id: 0
num_canonical_nodes: 128
icl_tasks: eval/yamls/tasks.yaml
eval_gauntlet: eval/yamls/eval_gauntlet.yaml
icl_seq_len: 2048
train_loader:
  name: text
  drop_last: true
  num_workers: 1
  dataset:
    cache_limit: 2tb
    download_timeout: 360
    eos_token_id: ${eos_token_id}
    max_seq_len: ${max_seq_len}
    num_canonical_nodes: ${num_canonical_nodes}
    shuffle: true
    sampling_method: fixed
    shuffle_algo: py1br
    predownload: 128
    shuffle_block_size: 4194304
    shuffle_seed: ${global_seed}
    local: /tmp/dataset/train
    remote: s3://data-force-one-datasets/__unitystorage/catalogs/36798a58-e180-4029-8cd7-842e61841ef0/volumes/b9e4994e-997d-4cbf-b76b-e38ff5533785/mpt/gpt4-tiktoken-seqlen-2048/26B-total-available-holdout-tokens-partition-sd-17/holdout/26B-tokens-from-1.0-passes/combined/mds
    split: null
dist_timeout: 1800
