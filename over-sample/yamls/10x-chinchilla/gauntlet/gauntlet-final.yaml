run_name: eval-final-mpt1.5-3b-params-52b-tokens-1.4x-epoch-raw-proportion 
name: eval-final-zack-multi-epoch-mpt2
gpu_type: a100_80gb
gpu_num: 8
cpus: null
platform: null
cluster: r1z1
image: mosaicml/llm-foundry:1.13.1_cu117-latest
partitions: null
optimization_level: null
integrations:
- integration_type: git_repo
  git_repo: zankner/llm-foundry
  git_branch: zack/mpt-over-sampling
  pip_install: -e ".[gpu]"
  ssh_clone: false
- integration_type: pip_packages
  packages:
  - oci-cli
  - slack-sdk
env_variables: []
scheduling: {}
compute:
  cluster: null
  gpu_type: null
metadata: {}
command: '

  cd llm-foundry/scripts

  composer eval/eval.py /mnt/config/parameters.yaml

  '
parameters:
  dist_timeout: 6000
  seed: 17
  max_seq_len: 2048
  device_eval_batch_size: 8
  precision: amp_bf16
  loggers:
    wandb:
      project: mpt-over-sampling
      group: mpt1.5-3b-params-52b-tokens-1.4x-epoch-raw-proportion
  callbacks:
    slack_logger: {}
  models:
  - model_name: eval-final-mpt1.5-3b-params-52b-tokens-1.4x-epoch-raw-proportion
    load_path: oci://mosaicml-internal-checkpoints/zack/mpt-oversample/mpt1.5-3b-params-52b-tokens-1.4x-epoch-raw-proportion-sd-17/latest-rank0.pt.symlink
  fsdp_config:
    activation_checkpointing: false
    activation_checkpointing_reentrant: false
    activation_cpu_offload: false
    limit_all_gathers: true
    mixed_precision: PURE
    sharding_strategy: FULL_SHARD
    state_dict_type: full
    verbose: false
  icl_tasks: eval/yamls/tasks.yaml
  model_gauntlet: eval/yamls/model_gauntlet.yaml
  run_name: eval-final-mpt1.5-3b-params-52b-tokens-1.4x-epoch-raw-proportion
  tokenizer:
    name: EleutherAI/gpt-neox-20b
    kwargs:
      model_max_length: ${max_seq_len}
  model:
    name: mpt_causal_lm
    init_device: meta
    d_model: 2560
    n_heads: 32
    n_layers: 32
    expansion_ratio: 4
    max_seq_len: ${max_seq_len}
    vocab_size: 50368 # update for hero run with custom tokenizer
    no_bias: true
    attn_config:
      alibi: true
      attn_impl: triton
      clip_qkv: 6
      attn_uses_sequence_id: false
entrypoint: ''
